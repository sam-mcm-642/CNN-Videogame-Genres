{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convolutional Neural Network to Predict Videogame Tags\n",
    "\n",
    "You're proboably going to need to install some of the packages below to run the program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! pip install keras-tuner\n",
    "#! pip install fast_ml\n",
    "#! pip install iterative-stratification\n",
    "#! python --version\n",
    "#! pip install --upgrade tensorflow\n",
    "#! pip show tensorflow\n",
    "#! pip install tensorflow-metal\n",
    "#! pip install opencv-python\n",
    "##change test igvjsfiunfi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Library import *\n",
    "import tensorflow as tf\n",
    "\n",
    "##Model building\n",
    "from keras import layers\n",
    "from keras.layers import Flatten, Dense, Dropout, BatchNormalization\n",
    "from keras.optimizers import Adam, Adagrad\n",
    "from keras.models import Sequential\n",
    "from keras.losses import BinaryCrossentropy, BinaryFocalCrossentropy, CategoricalCrossentropy\n",
    "import keras_tuner\n",
    "\n",
    "##Preprocessing/splitting\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from iterstrat.ml_stratifiers import MultilabelStratifiedShuffleSplit\n",
    "from keras.src.legacy.preprocessing.image import ImageDataGenerator\n",
    "#from keras.preprocessing.image import ImageDataGenerator\n",
    "import os\n",
    "import cv2\n",
    "from skimage.transform import resize\n",
    "\n",
    "##Results\n",
    "from sklearn.metrics import classification_report\n",
    "from keras.metrics import F1Score, FalseNegatives, FalsePositives, Recall, Precision\n",
    "\n",
    "##Misc\n",
    "from tqdm import tqdm ##progress bar for running code\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14687\n"
     ]
    }
   ],
   "source": [
    "##LOADING IN THE DATA\n",
    "Games = pd.read_json('https://seppe.net/aa/assignment2/dataset.json')\n",
    "print(len(Games)) ##14,687 games\n",
    "\n",
    "\n",
    "##Creating separate binary columns for all tags\n",
    "mlb = MultiLabelBinarizer()\n",
    "Games_tags_df = mlb.fit_transform(Games['tags'])\n",
    "tags_df = pd.DataFrame(Games_tags_df, columns=mlb.classes_)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1980s            591\n",
      "1990's           760\n",
      "2.5D             491\n",
      "2D              5927\n",
      "2D Fighter       287\n",
      "                ... \n",
      "World War I       32\n",
      "World War II     111\n",
      "Wrestling         21\n",
      "Zombies          578\n",
      "eSports          196\n",
      "Length: 441, dtype: int64\n",
      "28\n",
      "39\n",
      "14648\n",
      "14646\n",
      "14646\n"
     ]
    }
   ],
   "source": [
    "##FILTERING OUT TAGS/REMOVING DUPLICATE SCREENSHOTS\n",
    "List_of_all_tags = mlb.classes_\n",
    "# print(List_of_all_tags)\n",
    "\n",
    "#Sum up occurrences of each tag\n",
    "tag_counts = tags_df.sum()\n",
    "print(tag_counts)\n",
    "pd.options.display.max_rows = 445 ##to see all rows in the output\n",
    "# print(tag_counts)\n",
    "# print(len(tag_counts))\n",
    "\n",
    "# Split tags into lists of common and rare\n",
    "rare_tags = tag_counts[tag_counts < 100].index.tolist()\n",
    "common_tags = tag_counts[tag_counts > 100].index.tolist()\n",
    "\n",
    "##Common tags\n",
    "# print(\"Tags that appear more than 100 times:\")\n",
    "# print(common_tags)\n",
    "#print(len(common_tags))\n",
    "\n",
    "##Rare tags\n",
    "# print(\"Tags that appear less than 100 times:\")\n",
    "# print(rare_tags)\n",
    "#print(len(rare_tags))\n",
    "\n",
    "\n",
    "###53 tags manually selected based on a number of criteria\n",
    "##These were filtered down to 28 tags to try and improve performance\n",
    "##It is just the second list here we are using\n",
    "tags_to_use = ['2D', '3D', 'Action', 'Adventure', 'Anime', 'Arcade', \n",
    "               'Board Game', 'Card Game', 'Cartoony', 'Colorful', 'Combat', 'Cute', 'Dark', 'Driving',\n",
    "               'FPS', 'Family Friendly', 'Fantasy', 'First-Person', 'Horror', 'Mystery', 'Old School', \n",
    "               'Pixel Graphics', 'Platformer', 'Puzzle', 'RPG', 'Racing', 'Retro', 'Sci-fi', 'Sexual Content', 'Shooter', 'Side Scroller', 'Simulation', \n",
    "               'Sports', 'Strategy', 'Third Person', 'Top-Down', 'Violent']\n",
    "tags_to_use = ['2D', '3D', 'Action', 'Adventure', 'Anime', 'Arcade', \n",
    "               'Cartoony', 'Colorful', 'Combat', 'Cute', 'Dark',\n",
    "               'FPS', 'Family Friendly', 'Fantasy', 'First-Person', 'Horror', 'Mystery', \n",
    "               'Pixel Graphics', 'Puzzle', 'RPG', 'Retro', 'Sci-fi', 'Shooter', 'Simulation', \n",
    "               'Sports', 'Strategy', 'Third Person', 'Top-Down']\n",
    "print(len(tags_to_use))\n",
    "final_tags = tags_df[tags_to_use]\n",
    "\n",
    "\n",
    "\n",
    "##Need to check if any games are without tags after filtering tags\n",
    "no_final_tags = final_tags.loc[(final_tags==0).all(axis=1)]\n",
    "print(len(no_final_tags))\n",
    "###39 instances with no tagging of any of the final 28 tags\n",
    "##these instances can be removed (or maybe should be left in?)\n",
    "\n",
    "\n",
    "##Concatenating original videogame columns with binary columns of final tags\n",
    "Games_binary_tags = pd.concat([Games, final_tags], axis=1)\n",
    "##Filtering out games with no tags\n",
    "Games_binary_tags = Games_binary_tags[Games_binary_tags.iloc[:, 9:].sum(axis=1)!=0]\n",
    "\n",
    "print(len(Games_binary_tags))\n",
    "#print(Games_binary_tags.columns[9:])\n",
    "\n",
    "\n",
    "###Removing two instances that use duplicate screenshots of another instance (Collector's editions of the same game)\n",
    "Games_binary_tags = Games_binary_tags.drop(Games_binary_tags[Games_binary_tags['appid'] == '2131630,2131640,2131650,2131680,2306740'].index)\n",
    "Games_binary_tags = Games_binary_tags.drop(Games_binary_tags[Games_binary_tags['appid'] == '2131630,2131680'].index)\n",
    "\n",
    "\n",
    "Games = pd.DataFrame(Games_binary_tags, columns=['appid', 'screenshots', 'title', 'tags'])\n",
    "Tags = pd.DataFrame(Games_binary_tags, columns=tags_to_use)\n",
    "\n",
    "print(len(Games))\n",
    "print(len(Tags))\n",
    "\n",
    "\n",
    "###Dropped zombies, solitaire, logic, magic, hand-drawn, futuristic, cyberpunk, war, building, third person shooter, \n",
    "# top down shooter, exploration, 2d fighter, 2d platformer, 3d fighter, 3d platformer\n",
    "\n",
    "##Furtherly dropping any tags not in the second list above\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11672\n",
      "1495\n",
      "1479\n",
      "102069\n",
      "13336\n",
      "12846\n"
     ]
    }
   ],
   "source": [
    "##TRAIN-TEST-VALIDATION SPLIT\n",
    "## Splitting while trying to keep proprtion of positive to negative values equal \n",
    "## for all tags across the three datasets\n",
    "\n",
    "##This function is based on algorithm proposed in:\n",
    "# Sechidis K., Tsoumakas G., Vlahavas I. (2011) On the Stratification of\n",
    "# Multi-Label Data. In: Gunopulos D., Hofmann T., Malerba D., Vazirgiannis M.\n",
    "# (eds) Machine Learning and Knowledge Discovery in Databases. ECML PKDD\n",
    "# 2011. Lecture Notes in Computer Science, vol 6913. Springer, Berlin, Heidelberg.\n",
    "\n",
    "###Need to be np arrays\n",
    "Games_array = Games.to_numpy()\n",
    "Tags_array = Tags.to_numpy()\n",
    "\n",
    "msss = MultilabelStratifiedShuffleSplit(n_splits=1, test_size=.2, random_state=0)\n",
    "\n",
    "##train and test split based off of tags dataframe\n",
    "for train_index, test_index in msss.split(Games_array, Tags_array):\n",
    "    X_train, X_test = Games_array[train_index], Games_array[test_index]\n",
    "    y_train, y_test = Tags_array[train_index], Tags_array[test_index]\n",
    "    \n",
    "##now splitting test set into test and validation\n",
    "mssstv = MultilabelStratifiedShuffleSplit(n_splits=1, test_size=.5, random_state=0)\n",
    "\n",
    "##train and test split based off of tags dataframe\n",
    "for test_index, valid_index in mssstv.split(X_test, y_test):\n",
    "    X_valid, X_test = X_test[valid_index], X_test[test_index]\n",
    "    y_valid, y_test = y_test[valid_index], y_test[test_index]\n",
    "\n",
    "\n",
    "X_train = pd.DataFrame(X_train, columns=Games.columns)\n",
    "y_train = pd.DataFrame(y_train, columns =Tags.columns)\n",
    "\n",
    "X_valid = pd.DataFrame(X_valid, columns=Games.columns)\n",
    "y_valid = pd.DataFrame(y_valid, columns = Tags.columns)\n",
    "\n",
    "X_test = pd.DataFrame(X_test, columns=Games.columns)\n",
    "y_test = pd.DataFrame(y_test, columns =Tags.columns)\n",
    "\n",
    "Train_DF = pd.concat([X_train, y_train], axis=1)   \n",
    "Valid_DF = pd.concat([X_valid, y_valid], axis=1) \n",
    "Test_DF = pd.concat([X_test, y_test], axis=1) \n",
    "\n",
    "\n",
    "print(len(Train_DF))\n",
    "print(len(Valid_DF))\n",
    "print(len(Test_DF))\n",
    "\n",
    "##separate column for every screeshot\n",
    "Train_DF = Train_DF.explode('screenshots')\n",
    "Valid_DF = Valid_DF.explode('screenshots')\n",
    "Test_DF = Test_DF.explode('screenshots')\n",
    "\n",
    "\n",
    "print(len(Train_DF))\n",
    "print(len(Valid_DF))\n",
    "print(len(Test_DF))\n",
    "\n",
    "###To calculate number of steps per epoch later on\n",
    "N_train_instances = len(Train_DF)\n",
    "N_valid_instances = len(Valid_DF)\n",
    "N_test_instances = len(Test_DF)\n",
    "\n",
    "##Randomly shuffle Train_DF so screenshots aren't ordered by games (this will affect training outcomes)\n",
    "Train_DF = Train_DF.sample(frac=1).reset_index(drop=True)\n",
    "#print(len(Train_DF))\n",
    "#print(Train_DF.head(5))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "###IGNORE FOR NOW\n",
    "# ###Making dataframes where the true labels for each instance sum up to 1 (for a different loss function)\n",
    "# N_1_Train = Train_DF.iloc[:, 4:].sum(axis=1) ##n of true labels for each instance\n",
    "# CCE_Labels1 = Train_DF.iloc[:, 4:].astype(float).div(N_1_Train, axis=0)\n",
    "# Train_DF_CCE = pd.concat([Train_DF.iloc[:, :4], CCE_Labels1], axis=1)\n",
    "\n",
    "\n",
    "# N_1_Valid = Valid_DF.iloc[:, 4:].sum(axis=1) ##n of true labels for each instance\n",
    "# CCE_Labels2 = Valid_DF.iloc[:, 4:].astype(float).div(N_1_Valid, axis=0)\n",
    "# Valid_DF_CCE = pd.concat([Valid_DF.iloc[:, :4], CCE_Labels2], axis=1)\n",
    "\n",
    "# N_1_Test = Test_DF.iloc[:, 4:].sum(axis=1) ##n of true labels for each instance\n",
    "# CCE_Labels3 = Test_DF.iloc[:, 4:].astype(float).div(N_1_Test, axis=0)\n",
    "# Test_DF_CCE = pd.concat([Test_DF.iloc[:, :4], CCE_Labels3], axis=1)\n",
    "\n",
    "# print(Train_DF_CCE.head(5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "###CREATING AND SAVING 299X299 IMAGES FROM ORIGINAL IMAGES\n",
    "##Doing this every time we train a model/for every epoch is very computationally intense\n",
    "\n",
    "def preprocess_and_save_images(DF, source_dir, save_dir, target_size=(299, 299), crop_size=897):\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    for row in tqdm(DF.iterrows()):\n",
    "        # Read image\n",
    "        img_path = os.path.join(source_dir, row[1]['screenshots'])  # Access row data using row[1]\n",
    "        try:\n",
    "            img = cv2.imread(img_path)\n",
    "            # Resize\n",
    "            img = resize(img, target_size, anti_aliasing=True, preserve_range=True)\n",
    "\n",
    "            # Save preprocessed image with original filename\n",
    "            filename = os.path.basename(img_path)\n",
    "            save_path = os.path.join(save_dir, filename)\n",
    "            cv2.imwrite(save_path, img)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing image {img_path}: {e}\")\n",
    "\n",
    "source_directory = '/Users/sammcmanagan/AA Assignment 2/images'\n",
    "save_directory = '/Users/sammcmanagan/AA Assignment 2/resized_images'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7537it [11:37, 12.34it/s][ WARN:0@11849.580] global loadsave.cpp:248 findDecoder imread_('/Users/sammcmanagan/AA Assignment 2/images/1029210_ss_7096afc94036153001b266a75253fb6abdd03c54.1920x1080.jpg'): can't open/read file: check file path/integrity\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing image /Users/sammcmanagan/AA Assignment 2/images/1029210_ss_7096afc94036153001b266a75253fb6abdd03c54.1920x1080.jpg: 'NoneType' object has no attribute 'shape'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7543it [11:38, 10.31it/s][ WARN:0@11850.301] global loadsave.cpp:248 findDecoder imread_('/Users/sammcmanagan/AA Assignment 2/images/1029210_ss_ddf975e414f08a0bb419818d2cd94be4ebff6db9.1920x1080.jpg'): can't open/read file: check file path/integrity\n",
      "7545it [11:38, 12.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing image /Users/sammcmanagan/AA Assignment 2/images/1029210_ss_ddf975e414f08a0bb419818d2cd94be4ebff6db9.1920x1080.jpg: 'NoneType' object has no attribute 'shape'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "13336it [20:47, 10.69it/s]\n",
      "12846it [20:07, 10.64it/s]\n"
     ]
    }
   ],
   "source": [
    "#preprocess_and_save_images(Train_DF, source_directory, save_directory)\n",
    "#preprocess_and_save_images(Valid_DF, source_directory, save_directory)\n",
    "#preprocess_and_save_images(Test_DF, source_directory, save_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###LOADING IMAGES IN BATCHES\n",
    "###This whole step is so that the images are only loaded into memory and used for training in batches\n",
    "\n",
    "def preprocess_input_img(x):\n",
    "    x = keras.applications.xception.preprocess_input(x)\n",
    "    return x\n",
    "\n",
    "datagen = ImageDataGenerator(\n",
    "    preprocessing_function=preprocess_input_img ##This is a preprocessing function required by the Xception model\n",
    "    ##might be a good idea to add some data augmentation here\n",
    ")\n",
    "\n",
    "# Define batch size\n",
    "batch_size = 32\n",
    "\n",
    "# Create a generator for each dataset\n",
    "train_generator = datagen.flow_from_dataframe(\n",
    "    dataframe=Train_DF,\n",
    "    directory='/Users/sammcmanagan/AA Assignment 2/resized_images',\n",
    "    x_col=\"screenshots\",\n",
    "    y_col=tags_to_use,\n",
    "    batch_size=batch_size,\n",
    "    class_mode=\"raw\"\n",
    "    )\n",
    "\n",
    "valid_generator = datagen.flow_from_dataframe(\n",
    "    dataframe=Valid_DF,\n",
    "    directory='/Users/sammcmanagan/AA Assignment 2/resized_images',\n",
    "    x_col=\"screenshots\",\n",
    "    y_col=tags_to_use,\n",
    "    batch_size=batch_size,\n",
    "    class_mode=\"raw\"\n",
    "    )\n",
    "\n",
    "test_generator = datagen.flow_from_dataframe(\n",
    "    dataframe=Test_DF,\n",
    "    directory='/Users/sammcmanagan/AA Assignment 2/resized_images',\n",
    "    x_col=\"screenshots\",\n",
    "    y_col=tags_to_use,\n",
    "    batch_size=batch_size,\n",
    "    class_mode=\"raw\"\n",
    "    )\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'2D': 0.01450832640945486, '3D': 0.01395355677946979, 'Action': 0.011029206204191573, 'Adventure': 0.012431862079362655, 'Anime': 0.06276034189683335, 'Arcade': 0.03226075600713563, 'Cartoony': 0.05034102570182959, 'Colorful': 0.02091285759586932, 'Combat': 0.030214706611727895, 'Cute': 0.02820154441245368, 'Dark': 0.05146300417678502, 'FPS': 0.05852307118636985, 'Family Friendly': 0.040986926205908394, 'Fantasy': 0.027815558542137648, 'First-Person': 0.024078182287895867, 'Horror': 0.039429291966285515, 'Mystery': 0.05619572452775696, 'Pixel Graphics': 0.02767934687002761, 'Puzzle': 0.02667383945534195, 'RPG': 0.02658367215559579, 'Retro': 0.04448847389030263, 'Sci-fi': 0.04058836981374981, 'Shooter': 0.03472139211927446, 'Simulation': 0.022515598175934494, 'Sports': 0.0994184529204876, 'Strategy': 0.023980286195864474, 'Third Person': 0.038456833170802354, 'Top-Down': 0.03978779264115129}\n"
     ]
    }
   ],
   "source": [
    "###CALCULATING CLASS WEIGHTS FOR TAGS\n",
    "## This is to add more weight to tags that are poorly represented in the dataset\n",
    "class_freqs = Train_DF.iloc[:, 4:].sum()\n",
    "\n",
    "total_samples = len(Train_DF)\n",
    "inverse_frequencies = total_samples / class_freqs\n",
    "\n",
    "# Normalize weights\n",
    "class_weights = {class_label: weight / sum(inverse_frequencies) for class_label, weight in inverse_frequencies.items()}\n",
    "\n",
    "print(class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 6 Complete [02h 30m 26s]\n",
      "val_f1_score: 0.45277704298496246\n",
      "\n",
      "Best val_f1_score So Far: 0.512393057346344\n",
      "Total elapsed time: 14h 35m 53s\n",
      "\n",
      "Search: Running Trial #7\n",
      "\n",
      "Value             |Best Value So Far |Hyperparameter\n",
      "0.00015424        |0.00031141        |Learning Rate\n",
      "0.30739           |0.22047           |Dropout\n",
      "512               |384               |N_neurons_layer1\n",
      "256               |256               |N_neurons_layer2\n",
      "512               |512               |N_neurons_layer3\n",
      "128               |128               |N_neurons_layer4\n",
      "\n",
      "Epoch 1/3\n",
      "\u001b[1m3190/3190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1586s\u001b[0m 475ms/step - f1_score: 0.4121 - loss: 0.6143 - precision: 0.2396 - recall: 0.7931 - val_f1_score: 0.4910 - val_loss: 0.3889 - val_precision: 0.4469 - val_recall: 0.6028\n",
      "Epoch 2/3\n",
      "\u001b[1m3190/3190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1415s\u001b[0m 443ms/step - f1_score: 0.4843 - loss: 0.4029 - precision: 0.4244 - recall: 0.6046 - val_f1_score: 0.5043 - val_loss: 0.3841 - val_precision: 0.4534 - val_recall: 0.6131\n",
      "Epoch 3/3\n",
      "\u001b[1m2632/3190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m2:10:06\u001b[0m 14s/step - f1_score: 0.5092 - loss: 0.3870 - precision: 0.4437 - recall: 0.6317"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 69\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cnn\n\u001b[1;32m     56\u001b[0m tuner \u001b[38;5;241m=\u001b[39m keras_tuner\u001b[38;5;241m.\u001b[39mRandomSearch(\n\u001b[1;32m     57\u001b[0m     hypermodel \u001b[38;5;241m=\u001b[39m build_model,\n\u001b[1;32m     58\u001b[0m     objective\u001b[38;5;241m=\u001b[39m keras_tuner\u001b[38;5;241m.\u001b[39mObjective(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval_f1_score\u001b[39m\u001b[38;5;124m\"\u001b[39m, direction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     63\u001b[0m     overwrite\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     64\u001b[0m )\n\u001b[0;32m---> 69\u001b[0m tuner\u001b[38;5;241m.\u001b[39msearch(train_generator, \n\u001b[1;32m     70\u001b[0m              epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m, \n\u001b[1;32m     71\u001b[0m              validation_data\u001b[38;5;241m=\u001b[39mvalid_generator,\n\u001b[1;32m     72\u001b[0m              \u001b[38;5;66;03m#best_model = tuner.get_best_models()[0]\u001b[39;00m\n\u001b[1;32m     73\u001b[0m              )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/keras_tuner/src/engine/base_tuner.py:234\u001b[0m, in \u001b[0;36mBaseTuner.search\u001b[0;34m(self, *fit_args, **fit_kwargs)\u001b[0m\n\u001b[1;32m    231\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m    233\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_trial_begin(trial)\n\u001b[0;32m--> 234\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_try_run_and_update_trial(trial, \u001b[38;5;241m*\u001b[39mfit_args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_kwargs)\n\u001b[1;32m    235\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_trial_end(trial)\n\u001b[1;32m    236\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_search_end()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/keras_tuner/src/engine/base_tuner.py:274\u001b[0m, in \u001b[0;36mBaseTuner._try_run_and_update_trial\u001b[0;34m(self, trial, *fit_args, **fit_kwargs)\u001b[0m\n\u001b[1;32m    272\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_try_run_and_update_trial\u001b[39m(\u001b[38;5;28mself\u001b[39m, trial, \u001b[38;5;241m*\u001b[39mfit_args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_kwargs):\n\u001b[1;32m    273\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 274\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_and_update_trial(trial, \u001b[38;5;241m*\u001b[39mfit_args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_kwargs)\n\u001b[1;32m    275\u001b[0m         trial\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m=\u001b[39m trial_module\u001b[38;5;241m.\u001b[39mTrialStatus\u001b[38;5;241m.\u001b[39mCOMPLETED\n\u001b[1;32m    276\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/keras_tuner/src/engine/base_tuner.py:239\u001b[0m, in \u001b[0;36mBaseTuner._run_and_update_trial\u001b[0;34m(self, trial, *fit_args, **fit_kwargs)\u001b[0m\n\u001b[1;32m    238\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run_and_update_trial\u001b[39m(\u001b[38;5;28mself\u001b[39m, trial, \u001b[38;5;241m*\u001b[39mfit_args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_kwargs):\n\u001b[0;32m--> 239\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrun_trial(trial, \u001b[38;5;241m*\u001b[39mfit_args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_kwargs)\n\u001b[1;32m    240\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moracle\u001b[38;5;241m.\u001b[39mget_trial(trial\u001b[38;5;241m.\u001b[39mtrial_id)\u001b[38;5;241m.\u001b[39mmetrics\u001b[38;5;241m.\u001b[39mexists(\n\u001b[1;32m    241\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moracle\u001b[38;5;241m.\u001b[39mobjective\u001b[38;5;241m.\u001b[39mname\n\u001b[1;32m    242\u001b[0m     ):\n\u001b[1;32m    243\u001b[0m         \u001b[38;5;66;03m# The oracle is updated by calling `self.oracle.update_trial()` in\u001b[39;00m\n\u001b[1;32m    244\u001b[0m         \u001b[38;5;66;03m# `Tuner.run_trial()`. For backward compatibility, we support this\u001b[39;00m\n\u001b[1;32m    245\u001b[0m         \u001b[38;5;66;03m# use case. No further action needed in this case.\u001b[39;00m\n\u001b[1;32m    246\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    247\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe use case of calling \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    248\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`self.oracle.update_trial(trial_id, metrics)` \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    254\u001b[0m             stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[1;32m    255\u001b[0m         )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/keras_tuner/src/engine/tuner.py:314\u001b[0m, in \u001b[0;36mTuner.run_trial\u001b[0;34m(self, trial, *args, **kwargs)\u001b[0m\n\u001b[1;32m    312\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mappend(model_checkpoint)\n\u001b[1;32m    313\u001b[0m     copied_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcallbacks\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m callbacks\n\u001b[0;32m--> 314\u001b[0m     obj_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_and_fit_model(trial, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcopied_kwargs)\n\u001b[1;32m    316\u001b[0m     histories\u001b[38;5;241m.\u001b[39mappend(obj_value)\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m histories\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/keras_tuner/src/engine/tuner.py:233\u001b[0m, in \u001b[0;36mTuner._build_and_fit_model\u001b[0;34m(self, trial, *args, **kwargs)\u001b[0m\n\u001b[1;32m    231\u001b[0m hp \u001b[38;5;241m=\u001b[39m trial\u001b[38;5;241m.\u001b[39mhyperparameters\n\u001b[1;32m    232\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_try_build(hp)\n\u001b[0;32m--> 233\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhypermodel\u001b[38;5;241m.\u001b[39mfit(hp, model, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    235\u001b[0m \u001b[38;5;66;03m# Save the build config for model loading later.\u001b[39;00m\n\u001b[1;32m    236\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m backend\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mmulti_backend():\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/keras_tuner/src/engine/hypermodel.py:149\u001b[0m, in \u001b[0;36mHyperModel.fit\u001b[0;34m(self, hp, model, *args, **kwargs)\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, hp, model, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    126\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Train the model.\u001b[39;00m\n\u001b[1;32m    127\u001b[0m \n\u001b[1;32m    128\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[38;5;124;03m        If return a float, it should be the `objective` value.\u001b[39;00m\n\u001b[1;32m    148\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 149\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model\u001b[38;5;241m.\u001b[39mfit(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py:117\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/keras/src/backend/tensorflow/trainer.py:326\u001b[0m, in \u001b[0;36mTensorFlowTrainer.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[0m\n\u001b[1;32m    324\u001b[0m callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m    325\u001b[0m logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_function(iterator)\n\u001b[0;32m--> 326\u001b[0m callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_end(\n\u001b[1;32m    327\u001b[0m     step, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pythonify_logs(logs)\n\u001b[1;32m    328\u001b[0m )\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstop_training:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/keras/src/callbacks/callback_list.py:106\u001b[0m, in \u001b[0;36mCallbackList.on_train_batch_end\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m    104\u001b[0m logs \u001b[38;5;241m=\u001b[39m logs \u001b[38;5;129;01mor\u001b[39;00m {}\n\u001b[1;32m    105\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m callback \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallbacks:\n\u001b[0;32m--> 106\u001b[0m     callback\u001b[38;5;241m.\u001b[39mon_train_batch_end(batch, logs\u001b[38;5;241m=\u001b[39mlogs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/keras/src/callbacks/progbar_logger.py:58\u001b[0m, in \u001b[0;36mProgbarLogger.on_train_batch_end\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mon_train_batch_end\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch, logs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m---> 58\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_progbar(batch, logs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/keras/src/callbacks/progbar_logger.py:95\u001b[0m, in \u001b[0;36mProgbarLogger._update_progbar\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mseen \u001b[38;5;241m=\u001b[39m batch \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m  \u001b[38;5;66;03m# One-indexed.\u001b[39;00m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m---> 95\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprogbar\u001b[38;5;241m.\u001b[39mupdate(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mseen, \u001b[38;5;28mlist\u001b[39m(logs\u001b[38;5;241m.\u001b[39mitems()), finalize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/keras/src/utils/progbar.py:163\u001b[0m, in \u001b[0;36mProgbar.update\u001b[0;34m(self, current, values, finalize)\u001b[0m\n\u001b[1;32m    160\u001b[0m info \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m - \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[k], \u001b[38;5;28mlist\u001b[39m):\n\u001b[1;32m    162\u001b[0m     avg \u001b[38;5;241m=\u001b[39m backend\u001b[38;5;241m.\u001b[39mconvert_to_numpy(\n\u001b[0;32m--> 163\u001b[0m         backend\u001b[38;5;241m.\u001b[39mnumpy\u001b[38;5;241m.\u001b[39mmean(\n\u001b[1;32m    164\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[k][\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[k][\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m    165\u001b[0m         )\n\u001b[1;32m    166\u001b[0m     )\n\u001b[1;32m    167\u001b[0m     avg \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfloat\u001b[39m(avg)\n\u001b[1;32m    168\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mabs\u001b[39m(avg) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1e-3\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/keras/src/backend/tensorflow/numpy.py:489\u001b[0m, in \u001b[0;36mmean\u001b[0;34m(x, axis, keepdims)\u001b[0m\n\u001b[1;32m    483\u001b[0m         gather_indices \u001b[38;5;241m=\u001b[39m [i \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(rank) \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m axis]\n\u001b[1;32m    484\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mIndexedSlices(\n\u001b[1;32m    485\u001b[0m             tf\u001b[38;5;241m.\u001b[39mreduce_mean(x\u001b[38;5;241m.\u001b[39mvalues, axis\u001b[38;5;241m=\u001b[39maxis),\n\u001b[1;32m    486\u001b[0m             x\u001b[38;5;241m.\u001b[39mindices,\n\u001b[1;32m    487\u001b[0m             tf\u001b[38;5;241m.\u001b[39mgather(x\u001b[38;5;241m.\u001b[39mdense_shape, gather_indices, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m),\n\u001b[1;32m    488\u001b[0m         )\n\u001b[0;32m--> 489\u001b[0m x \u001b[38;5;241m=\u001b[39m convert_to_tensor(x)\n\u001b[1;32m    490\u001b[0m ori_dtype \u001b[38;5;241m=\u001b[39m standardize_dtype(x\u001b[38;5;241m.\u001b[39mdtype)\n\u001b[1;32m    491\u001b[0m compute_dtype \u001b[38;5;241m=\u001b[39m dtypes\u001b[38;5;241m.\u001b[39mresult_type(x\u001b[38;5;241m.\u001b[39mdtype, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfloat32\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/keras/src/backend/tensorflow/core.py:114\u001b[0m, in \u001b[0;36mconvert_to_tensor\u001b[0;34m(x, dtype, sparse)\u001b[0m\n\u001b[1;32m    112\u001b[0m         x \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mconvert_to_tensor(x)\n\u001b[1;32m    113\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mcast(x, dtype)\n\u001b[0;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mconvert_to_tensor(x, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m x\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m dtype:\n\u001b[1;32m    116\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x, tf\u001b[38;5;241m.\u001b[39mSparseTensor):\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/tensorflow/python/util/dispatch.py:1260\u001b[0m, in \u001b[0;36madd_dispatch_support.<locals>.decorator.<locals>.op_dispatch_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1258\u001b[0m \u001b[38;5;66;03m# Fallback dispatch system (dispatch v1):\u001b[39;00m\n\u001b[1;32m   1259\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1260\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m dispatch_target(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1261\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mTypeError\u001b[39;00m, \u001b[38;5;167;01mValueError\u001b[39;00m):\n\u001b[1;32m   1262\u001b[0m   \u001b[38;5;66;03m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[39;00m\n\u001b[1;32m   1263\u001b[0m   \u001b[38;5;66;03m# TypeError, when given unexpected types.  So we need to catch both.\u001b[39;00m\n\u001b[1;32m   1264\u001b[0m   result \u001b[38;5;241m=\u001b[39m dispatch(op_dispatch_handler, args, kwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/tensorflow/python/framework/tensor_conversion.py:161\u001b[0m, in \u001b[0;36mconvert_to_tensor_v2_with_dispatch\u001b[0;34m(value, dtype, dtype_hint, name)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;129m@tf_export\u001b[39m\u001b[38;5;241m.\u001b[39mtf_export(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconvert_to_tensor\u001b[39m\u001b[38;5;124m\"\u001b[39m, v1\u001b[38;5;241m=\u001b[39m[])\n\u001b[1;32m     97\u001b[0m \u001b[38;5;129m@dispatch\u001b[39m\u001b[38;5;241m.\u001b[39madd_dispatch_support\n\u001b[1;32m     98\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconvert_to_tensor_v2_with_dispatch\u001b[39m(\n\u001b[1;32m     99\u001b[0m     value, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, dtype_hint\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    100\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m tensor_lib\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[1;32m    101\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Converts the given `value` to a `Tensor`.\u001b[39;00m\n\u001b[1;32m    102\u001b[0m \n\u001b[1;32m    103\u001b[0m \u001b[38;5;124;03m  This function converts Python objects of various types to `Tensor`\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;124;03m    ValueError: If the `value` is a tensor not of given `dtype` in graph mode.\u001b[39;00m\n\u001b[1;32m    160\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[0;32m--> 161\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m convert_to_tensor_v2(\n\u001b[1;32m    162\u001b[0m       value, dtype\u001b[38;5;241m=\u001b[39mdtype, dtype_hint\u001b[38;5;241m=\u001b[39mdtype_hint, name\u001b[38;5;241m=\u001b[39mname\n\u001b[1;32m    163\u001b[0m   )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/tensorflow/python/framework/tensor_conversion.py:171\u001b[0m, in \u001b[0;36mconvert_to_tensor_v2\u001b[0;34m(value, dtype, dtype_hint, name)\u001b[0m\n\u001b[1;32m    169\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Converts the given `value` to a `Tensor`.\"\"\"\u001b[39;00m\n\u001b[1;32m    170\u001b[0m \u001b[38;5;66;03m# preferred_dtype = preferred_dtype or dtype_hint\u001b[39;00m\n\u001b[0;32m--> 171\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m tensor_conversion_registry\u001b[38;5;241m.\u001b[39mconvert(\n\u001b[1;32m    172\u001b[0m     value, dtype, name, preferred_dtype\u001b[38;5;241m=\u001b[39mdtype_hint\n\u001b[1;32m    173\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/tensorflow/python/framework/tensor_conversion_registry.py:234\u001b[0m, in \u001b[0;36mconvert\u001b[0;34m(value, dtype, name, as_ref, preferred_dtype, accepted_result_types)\u001b[0m\n\u001b[1;32m    225\u001b[0m       \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    226\u001b[0m           _add_error_prefix(\n\u001b[1;32m    227\u001b[0m               \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConversion function \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconversion_func\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m for type \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    230\u001b[0m               \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mactual = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mret\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mbase_dtype\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    231\u001b[0m               name\u001b[38;5;241m=\u001b[39mname))\n\u001b[1;32m    233\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ret \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 234\u001b[0m   ret \u001b[38;5;241m=\u001b[39m conversion_func(value, dtype\u001b[38;5;241m=\u001b[39mdtype, name\u001b[38;5;241m=\u001b[39mname, as_ref\u001b[38;5;241m=\u001b[39mas_ref)\n\u001b[1;32m    236\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ret \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28mNotImplemented\u001b[39m:\n\u001b[1;32m    237\u001b[0m   \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/tensorflow/python/framework/constant_tensor_conversion.py:29\u001b[0m, in \u001b[0;36m_constant_tensor_conversion_function\u001b[0;34m(v, dtype, name, as_ref)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mframework\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m constant_op  \u001b[38;5;66;03m# pylint: disable=g-import-not-at-top\u001b[39;00m\n\u001b[1;32m     28\u001b[0m _ \u001b[38;5;241m=\u001b[39m as_ref\n\u001b[0;32m---> 29\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m constant_op\u001b[38;5;241m.\u001b[39mconstant(v, dtype\u001b[38;5;241m=\u001b[39mdtype, name\u001b[38;5;241m=\u001b[39mname)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/tensorflow/python/ops/weak_tensor_ops.py:142\u001b[0m, in \u001b[0;36mweak_tensor_binary_op_wrapper.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    141\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ops\u001b[38;5;241m.\u001b[39mis_auto_dtype_conversion_enabled():\n\u001b[0;32m--> 142\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m op(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    143\u001b[0m   bound_arguments \u001b[38;5;241m=\u001b[39m signature\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    144\u001b[0m   bound_arguments\u001b[38;5;241m.\u001b[39mapply_defaults()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/tensorflow/python/framework/constant_op.py:276\u001b[0m, in \u001b[0;36mconstant\u001b[0;34m(value, dtype, shape, name)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;129m@tf_export\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconstant\u001b[39m\u001b[38;5;124m\"\u001b[39m, v1\u001b[38;5;241m=\u001b[39m[])\n\u001b[1;32m    178\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconstant\u001b[39m(\n\u001b[1;32m    179\u001b[0m     value, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, shape\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConst\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    180\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[ops\u001b[38;5;241m.\u001b[39mOperation, ops\u001b[38;5;241m.\u001b[39m_EagerTensorBase]:\n\u001b[1;32m    181\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Creates a constant tensor from a tensor-like object.\u001b[39;00m\n\u001b[1;32m    182\u001b[0m \n\u001b[1;32m    183\u001b[0m \u001b[38;5;124;03m  Note: All eager `tf.Tensor` values are immutable (in contrast to\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    274\u001b[0m \u001b[38;5;124;03m    ValueError: if called on a symbolic tensor.\u001b[39;00m\n\u001b[1;32m    275\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[0;32m--> 276\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m _constant_impl(value, dtype, shape, name, verify_shape\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    277\u001b[0m                         allow_broadcast\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/tensorflow/python/framework/constant_op.py:289\u001b[0m, in \u001b[0;36m_constant_impl\u001b[0;34m(value, dtype, shape, name, verify_shape, allow_broadcast)\u001b[0m\n\u001b[1;32m    287\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m trace\u001b[38;5;241m.\u001b[39mTrace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf.constant\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    288\u001b[0m       \u001b[38;5;28;01mreturn\u001b[39;00m _constant_eager_impl(ctx, value, dtype, shape, verify_shape)\n\u001b[0;32m--> 289\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m _constant_eager_impl(ctx, value, dtype, shape, verify_shape)\n\u001b[1;32m    291\u001b[0m const_tensor \u001b[38;5;241m=\u001b[39m ops\u001b[38;5;241m.\u001b[39m_create_graph_constant(  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m    292\u001b[0m     value, dtype, shape, name, verify_shape, allow_broadcast\n\u001b[1;32m    293\u001b[0m )\n\u001b[1;32m    294\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m const_tensor\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/tensorflow/python/framework/constant_op.py:301\u001b[0m, in \u001b[0;36m_constant_eager_impl\u001b[0;34m(ctx, value, dtype, shape, verify_shape)\u001b[0m\n\u001b[1;32m    297\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_constant_eager_impl\u001b[39m(\n\u001b[1;32m    298\u001b[0m     ctx, value, dtype, shape, verify_shape\n\u001b[1;32m    299\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ops\u001b[38;5;241m.\u001b[39m_EagerTensorBase:\n\u001b[1;32m    300\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Creates a constant on the current device.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 301\u001b[0m   t \u001b[38;5;241m=\u001b[39m convert_to_eager_tensor(value, ctx, dtype)\n\u001b[1;32m    302\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m shape \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    303\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/tensorflow/python/framework/constant_op.py:108\u001b[0m, in \u001b[0;36mconvert_to_eager_tensor\u001b[0;34m(value, ctx, dtype)\u001b[0m\n\u001b[1;32m    106\u001b[0m     dtype \u001b[38;5;241m=\u001b[39m dtypes\u001b[38;5;241m.\u001b[39mas_dtype(dtype)\u001b[38;5;241m.\u001b[39mas_datatype_enum\n\u001b[1;32m    107\u001b[0m ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m--> 108\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ops\u001b[38;5;241m.\u001b[39mEagerTensor(value, ctx\u001b[38;5;241m.\u001b[39mdevice_name, dtype)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "###MODEL BUILDING\n",
    "##Using keras tuner for choosing hyperparameters\n",
    "def build_model(hp):\n",
    "    ###all the hyperparameters tuned by keras tuner\n",
    "    learn_rate = hp.Float(\"Learning Rate\", min_value=1e-5, max_value=1e-2, sampling=\"log\")\n",
    "    dropout_rate = hp.Float(\"Dropout\", min_value=.2, max_value=.5, sampling=\"log\")\n",
    "    N_neurons1 = hp.Int(\"N_neurons_layer1\", min_value=384, max_value=512, step=128)\n",
    "    N_neurons2 = hp.Int(\"N_neurons_layer2\", min_value=256, max_value=512, step=128)\n",
    "    N_neurons3 = hp.Int(\"N_neurons_layer3\", min_value=256, max_value=512, step=128)\n",
    "    N_neurons4 = hp.Int(\"N_neurons_layer4\", min_value=128, max_value=512, step=128)\n",
    "    #activation_fun = hp.Choice(\"Activation function\", [\"relu\"])\n",
    "    #loss_fun = hp.Choice(\"Loss function\", [BinaryCrossentropy(), BinaryFocalCrossentropy()])\n",
    "    #optimizer = hp.Choice(\"Optimizer\", [Adam, Adagrad]), can't use adagrad\n",
    "    \n",
    "    \n",
    "    cnn = Sequential()\n",
    "\n",
    "    Xception = keras.applications.Xception(include_top=False, weights=\"imagenet\")\n",
    "    for layer in Xception.layers:\n",
    "        layer.trainable=False  ##Freeze all Xception layers so they aren't trained\n",
    "    cnn.add(Xception)\n",
    "    cnn.add(Flatten())\n",
    "    \n",
    "    ##Fully connected layer 1\n",
    "    cnn.add(Dense(N_neurons1, activation= 'relu'))\n",
    "    cnn.add(BatchNormalization())\n",
    "    cnn.add(Dropout(dropout_rate))\n",
    "    ##Fully connected layer 2\n",
    "    cnn.add(Dense(N_neurons2, activation= 'relu'))\n",
    "    cnn.add(BatchNormalization())\n",
    "    cnn.add(Dropout(dropout_rate))\n",
    "    ##Fully connected layer 3   \n",
    "    cnn.add(Dense(N_neurons3, activation= 'relu'))\n",
    "    cnn.add(BatchNormalization())\n",
    "    cnn.add(Dropout(.2))\n",
    "    ##Fully connected layer 4   \n",
    "    cnn.add(Dense(N_neurons4, activation= 'relu'))\n",
    "    cnn.add(BatchNormalization())\n",
    "    cnn.add(Dropout(.2))   \n",
    "\n",
    "    \n",
    "    ##Output layer (sigmoid instead of softmax, as all outputs should be independent pdfs)\n",
    "    cnn.add(Dense(28, activation='sigmoid'))\n",
    "    \n",
    "    cnn.compile(\n",
    "        optimizer = Adam(learning_rate=learn_rate),\n",
    "        loss= \"binary_crossentropy\",\n",
    "        metrics = [F1Score(average=\"weighted\", threshold=.25), Precision(thresholds=.25), Recall(thresholds=.25)]\n",
    "    )\n",
    "    #print(Xception.summary())\n",
    "    #print(cnn.summary())    \n",
    "    return cnn\n",
    "\n",
    "\n",
    "\n",
    "tuner = keras_tuner.RandomSearch(\n",
    "    hypermodel = build_model,\n",
    "    objective= keras_tuner.Objective(\"val_f1_score\", direction=\"max\"), ##May be better to use val_loss\n",
    "    directory='/Users/sammcmanagan/Library/Mobile Documents/com~apple~CloudDocs/Documents/M.Sc Statistics & Data Science/Advanced Analytics/Assignment 2/Tuner Results',\n",
    "    executions_per_trial=2, ##Each combination of parameters tested twice\n",
    "    max_trials=15, ##Test 15 different combinations of hyperparameters\n",
    "    project_name=\"CNN Hyperparameters\",\n",
    "    #overwrite=True ##This deletes results from previous tuner searches\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "tuner.search(train_generator, \n",
    "             epochs=3, \n",
    "             validation_data=valid_generator,\n",
    "             #best_model = tuner.get_best_models()[0]\n",
    "             )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###REPORT FOR BINARY CROSS ENTROPY (BCE) MODEL\n",
    "\n",
    "# Get the predicted probabilities for the validation/test set\n",
    "valid_generator.reset()\n",
    "y_pred_prob = cnn.predict(valid_generator)\n",
    "\n",
    "# Get the ground truth labels for the validation/test set\n",
    "y_true = valid_generator.labels\n",
    "\n",
    "# Threshold the predicted probabilities to obtain binary predictions\n",
    "y_pred = (y_pred_prob > 0.2).astype(int) ##Probably optimal around .2 for most tags\n",
    "\n",
    "# Calculate classification report\n",
    "report = classification_report(y_true, y_pred, target_names=tags_to_use)\n",
    "\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##//////////////////"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##OPTIMISING F1 SCORE FOR EACH TAG WITH INDIVIDUAL THRESHOLDS\n",
    "##All of this code (and all of the code from the cells below) is just copied straight from\n",
    "## https://github.com/vanHavel/games-cnn/tree/master/predict\n",
    "##It will need to be tailored to ouyr project\n",
    "\n",
    "\n",
    "# get optimal thresholds for each label on the training data, maximizing F1 score\n",
    "# model_path: path to keras model\n",
    "def get_cutoffs(model_path=os.path.join('checkpoints', 'xception_trained')):\n",
    "    \n",
    "    # load training data\n",
    "    train_X = np.load(os.path.join('training_data','train_X.npy'))\n",
    "    y_true = np.load(os.path.join('training_data', 'train_Y.npy'))\n",
    "\n",
    "    # load model\n",
    "    model = load_model(model_path)\n",
    "    \n",
    "    # get predictions\n",
    "    y_pred = model.predict(train_X, batch_size=16)\n",
    "    \n",
    "    # calculate optimal cutoff point for each label separately\n",
    "    k = np.shape(y_true)[1]\n",
    "    cutoffs = np.zeros(k)\n",
    "    for label_id in range(k):\n",
    "        print(\"label \" + str(label_id))\n",
    "        # get candidates\n",
    "        possible_cutoffs = y_pred[:,label_id]\n",
    "        best_cutoff = -1\n",
    "        best_f1 = -1\n",
    "        # try each candidate, comparing f1 scores\n",
    "        possible_cutoffs = np.sort(possible_cutoffs)\n",
    "        for i in range(0, len(possible_cutoffs)):\n",
    "            candidate = possible_cutoffs[i]\n",
    "            f1_score = measures.get_f1_score_for_label(label_id, y_pred, y_true, cutoff=candidate)\n",
    "            if f1_score > best_f1:\n",
    "                best_f1 = f1_score\n",
    "                best_cutoff = candidate\n",
    "        cutoffs[label_id] = best_cutoff\n",
    "        print(best_cutoff)\n",
    "        \n",
    "    # write cutoffs\n",
    "    np.save(os.path.join('cutoffs', 'cutoffs.npy'), cutoffs)\n",
    "    \n",
    "get_cutoffs()\n",
    "\n",
    "\n",
    "\n",
    "###I think this is the only code we will actually need to use,\n",
    "###the code below we can already do with what we have"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import measures\n",
    "\n",
    "# evaluate a given model on test data\n",
    "# model_path: path to keras model\n",
    "# cutoff_file: path to threshold file\n",
    "def evaluate(model_path=os.path.join('model', 'mod'),\n",
    "             cutoff_file='cutoffs.npy'):\n",
    "\n",
    "    # load test data\n",
    "    test_X = np.load(os.path.join('training_data','test_X.npy'))\n",
    "    test_Y = np.load(os.path.join('training_data', 'test_Y.npy'))\n",
    "\n",
    "    # load model\n",
    "    model = model = load_model(model_path)\n",
    "    \n",
    "    # load cutoffs\n",
    "    cutoffs = np.load(os.path.join('cutoffs', cutoff_file))\n",
    "\n",
    "    # get predictions\n",
    "    test_predictions = model.predict(test_X, batch_size=16)\n",
    "\n",
    "    # get measures\n",
    "    test_measures = measures.get_measures(test_predictions, test_Y, cutoffs)\n",
    "       \n",
    "    # read genres\n",
    "    genre_file_path = os.path.join('training_data', 'genres.txt')\n",
    "    with open(genre_file_path, 'r') as handler:\n",
    "        genres = handler.readlines()\n",
    "    genres = [genre[:-1] for genre in genres]\n",
    "    \n",
    "    # print measures \n",
    "    print(\"Statistics on test data:\")\n",
    "    measures.print_measures(test_measures, genres)    \n",
    "\n",
    "evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# calculate the f1_score for a specific label component\n",
    "def get_f1_score_for_label(label_id, y_pred, y_true, cutoff=0.5):\n",
    "    # project to component\n",
    "    y_pred = y_pred[:,label_id]\n",
    "    y_true = y_true[:,label_id]\n",
    "    \n",
    "    # transform predictions to binary matrix\n",
    "    fun = lambda x: 1 if x >= cutoff else 0\n",
    "    vfunc = np.vectorize(fun)\n",
    "    y_pred = vfunc(y_pred)\n",
    "    \n",
    "    # calculate f1 score\n",
    "    true_and_predicted = np.sum(y_true * y_pred)\n",
    "    true = np.sum(y_true)\n",
    "    predicted = np.sum(y_pred)\n",
    "    if predicted == 0 or true == 0:\n",
    "        return -1\n",
    "    recall = true_and_predicted / true\n",
    "    precision = true_and_predicted / predicted\n",
    "    f1_score = 2 * (recall * precision) / (recall + precision)\n",
    "    \n",
    "    return f1_score\n",
    "    \n",
    "# get all kinds of accuracy measures for predicted labels\n",
    "def get_measures(y_pred, y_true, cutoffs):\n",
    "    # transform predictions to binary matrix\n",
    "    y_pred = transform_to_binary(y_pred, cutoffs)\n",
    "    \n",
    "    # get input sizes\n",
    "    n = np.shape(y_pred)[0]\n",
    "    k = np.shape(y_pred)[1]\n",
    "    \n",
    "    # create result data structure\n",
    "    measures = dict()\n",
    "    \n",
    "    # basic measures\n",
    "    measures['label_cardinality'] = np.sum(y_true) / n\n",
    "    measures['label_density'] = measures['label_cardinality'] / k\n",
    "    \n",
    "    # elementary statistics\n",
    "    zero_one_error = sum([(1 if (y_true[i] == y_pred[i]).all() else 0) for i in range(np.shape(y_true)[0])]) / n\n",
    "    true_and_predicted = np.sum(y_true * y_pred, axis=0)\n",
    "    true = np.sum(y_true, axis=0)\n",
    "    predicted = np.sum(y_pred, axis=0)\n",
    "    recall = true_and_predicted / true\n",
    "    precision = true_and_predicted / predicted\n",
    "    f1_score = 2 * (precision * recall) / (precision + recall)\n",
    "    \n",
    "    measures['zero_one_error'] = zero_one_error\n",
    "    measures['recall'] = recall\n",
    "    measures['precision'] = precision\n",
    "    measures['f1_score'] = f1_score\n",
    "    \n",
    "    # global statistics\n",
    "    global_recall = np.sum(true_and_predicted) / np.sum(true)\n",
    "    global_precision = np.sum(true_and_predicted) / np.sum(predicted)\n",
    "    global_f1_score = 2 * (global_precision * global_recall) / (global_precision + global_recall)\n",
    "    \n",
    "    measures['global_recall'] = global_recall\n",
    "    measures['global_precision'] = global_precision\n",
    "    measures['global_f1_score'] = global_f1_score\n",
    "    \n",
    "    # average statistics\n",
    "    measures['average_recall'] = np.mean(recall)\n",
    "    measures['average_precision'] = np.mean(precision)\n",
    "    measures['average_f1_score'] = np.mean(f1_score)\n",
    "    \n",
    "    return measures\n",
    "\n",
    "# transform predictions to binary matrix, given cutoff\n",
    "def transform_to_binary(y_pred, cutoffs):\n",
    "    # get input sizes\n",
    "    n = np.shape(y_pred)[0]\n",
    "    k = np.shape(y_pred)[1]\n",
    "    \n",
    "    # transform to binary according to cutoffs\n",
    "    for i in range(n):\n",
    "        for j in range(k):\n",
    "            y_pred[i][j] = 1 if y_pred[i][j] >= cutoffs[j] else 0\n",
    "    \n",
    "    return y_pred\n",
    "\n",
    "# print measures given genre names   \n",
    "def print_measures(measures, genres):\n",
    "    print(\"Label cardinality: \" + str(measures['label_cardinality']))\n",
    "    print(\"Label density: \" + str(measures['label_density']))\n",
    "    print(\"\")\n",
    "    print(\"Zero-one error: \" + str(measures['zero_one_error']))\n",
    "    print(\"Global precision: \" + str(measures['global_precision']))\n",
    "    print(\"Global recall: \" + str(measures['global_recall']))\n",
    "    print(\"Global F1 score: \" + str(measures['global_f1_score']))\n",
    "    print(\"\")\n",
    "    print(\"Average precision: \" + str(measures['average_precision']))\n",
    "    print(\"Average recall: \" + str(measures['average_recall']))\n",
    "    print(\"Average F1 score: \" + str(measures['average_f1_score']))\n",
    "    print(\"\")\n",
    "    print(\"Precision per genre:\")\n",
    "    for i in range(len(genres)):\n",
    "        print(genres[i] + \": \" + str(measures['precision'][i]))\n",
    "    print(\"\")\n",
    "    print(\"Recall per genre:\")\n",
    "    for i in range(len(genres)):\n",
    "        print(genres[i] + \": \" + str(measures['recall'][i]))\n",
    "    print(\"\")\n",
    "    print(\"F1 score per genre:\")\n",
    "    for i in range(len(genres)):\n",
    "        print(genres[i] + \": \" + str(measures['f1_score'][i]))\n",
    "\n",
    "# save precision, recall and F1 score\n",
    "def save_measures(measures):\n",
    "    np.save('precision.npy', measures['precision'])\n",
    "    np.save('recall.npy', measures['recall'])\n",
    "    np.save('f1.npy', measures['f1_score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "from keras.applications import imagenet_utils\n",
    "from keras.applications.inception_v3 import preprocess_input as preprocess_xception\n",
    "from keras.preprocessing.image import load_img, img_to_array\n",
    "import numpy as np\n",
    "import os\n",
    "import ast\n",
    "\n",
    "# classify some images\n",
    "# image_paths: lists of paths to jpg images\n",
    "# model_path: path to keras model\n",
    "# cutoff_file: path to threshold file\n",
    "def classify_image(image_paths=['img.jpg'],\n",
    "    model_path=os.path.join('checkpoints', 'xception_trained'),\n",
    "    cutoff_file='cutoffs.npy'):\n",
    "    # load model\n",
    "    model = load_model(model_path)\n",
    "\n",
    "    # read genre file \n",
    "    genre_file_path = os.path.join('training_data', 'genres.txt')\n",
    "    with open(genre_file_path, 'r') as handler:\n",
    "        genres = handler.readlines()\n",
    "\n",
    "    # determine preprocess method\n",
    "    preprocess_path = os.path.join('training_data', 'preprocess.txt')\n",
    "    with open(preprocess_path, 'r') as preprocess_file:\n",
    "        dictionary = ast.literal_eval(preprocess_file.read())\n",
    "        preprocess_method = dictionary['preprocess']\n",
    "    if preprocess_method == 'xception':\n",
    "        preprocess = preprocess_xception\n",
    "    elif preprocess_method == 'vgg':\n",
    "        preprocess = imagenet_utils.preprocess_input\n",
    "    elif preprocess_method == 'none':\n",
    "        preprocess = lambda x:x\n",
    "\n",
    "    # preprocess images\n",
    "    input_shape = model.layers[0].input_shape\n",
    "    dimension = (input_shape[1], input_shape[2])\n",
    "    screenshots = [process_screen(image_path, dimension, preprocess) for image_path in image_paths]\n",
    "\n",
    "    # load cutoffs\n",
    "    cutoffs = np.load(os.path.join('cutoffs', cutoff_file))\n",
    "\n",
    "    # predict classes\n",
    "    predictions = model.predict(np.array(screenshots))\n",
    "    for prediction in predictions:\n",
    "        print(prediction)\n",
    "        classes = [i for i in range(0, len(prediction)) if prediction[i] >= cutoffs[i]]\n",
    "        print('Predicted genres:')\n",
    "        for c in classes:\n",
    "            print(genres[c][:-1])\n",
    "\n",
    "# preprocess a single screen\n",
    "def process_screen(screen_file, dimension, preprocess):\n",
    "    screenshot = load_img(screen_file, target_size=dimension)\n",
    "    screenshot = img_to_array(screenshot)\n",
    "    screenshot = np.expand_dims(screenshot, axis=0)\n",
    "    screenshot = preprocess(screenshot)\n",
    "    screenshot = screenshot[0]\n",
    "    return screenshot\n",
    "\n",
    "classify_image()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "221ab5ba4d875600bd6bb0e7eee0cec001efdea31b25afc3ad4fb9d8815d7c77"
  },
  "kernelspec": {
   "display_name": "Python 3.11.5 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nb